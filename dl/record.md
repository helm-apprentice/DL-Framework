梯度下降法和牛顿法都是用来求函数极值的方法,主要的区别有:
1. 梯度下降法只利用了目标函数的一阶导数信息,每次沿着目标函数的梯度方向迭代,步长通过线搜索确定。而牛顿法利用了目标函数的二阶导数信息,可以更快地逼近极值点。
2. 梯度下降法的迭代速度比较慢,特别是当接近极小值点时。而牛顿法的迭代速度更快,通常只需要很少的迭代步数就可以达到高精度。
3. 梯度下降法对初始点的选择不是很敏感。而牛顿法对初始点的选择很敏感,如果初始点远离极值点,可能不收敛。
4. 梯度下降法计算量小,每次只需要计算一阶导数。牛顿法需要计算二阶导数,计算量较大。

在牛顿法中,目标函数在迭代点附近使用二次 Taylor 展开逼近,这就是二次近似函数。具体而言是:
f(x) ≈ f(x_k) + ∇f(x_k)^T(x - x_k) + 1/2(x - x_k)^T∇^2f(x_k)(x - x_k)
让二次近似函数的导数等于0可以求得下一个迭代点x_{k+1}。
重复这一过程,不断用二次近似函数逼近目标函数,可以快速逼近极小值点。二次近似利用了目标函数的二阶导数信息,使牛顿法收敛速度快于梯度下降法。

假设原函数f(x)在x_0附近的二次Taylor展开为:
f(x) ≈ f(x_0) + f'(x_0)(x - x_0) + 1/2 f''(x_0)(x - x_0)^2
对该式求导得到:
f'(x) ≈ f'(x_0) + f''(x_0)(x - x_0)

牛顿法在迭代点附近使用二次Taylor展开来近似原始函数,而在这个二次 Taylor 展开中,会忽略三阶及更高阶导数项。
所以在求二次近似函数的导数时,理论上本应该出现三阶导数项,但因为牛顿法做了忽略高阶项的近似,三阶导数被去掉了。
这就是导数里不会出现三阶导数的原因 - 因为牛顿法做了只保留到二阶导数的近似。

如果按完整的Taylor展开来看,求二次近似的导数等于0时,理应还有三阶导数项的影响。
之所以我们在牛顿法里忽略了三阶和更高阶的导数项,是因为我们假设在迭代点x_k附近,f(x)可以被其二阶Taylor展开很好地逼近。也就是说二阶导数项已经能反映函数在该点的特征,高阶导数项的影响很小,可以忽略。
这是牛顿法做出的一种近似处理。在大多数实际问题中,这种只保留到二阶导数的近似是可行的。
